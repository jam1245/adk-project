{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c1644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff5167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '-': Expected package name at the start of dependency specifier\n",
      "    -\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==5.28.1 google-adk==1.0.0 litellm -q -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81487d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.invalidate_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_BASE\"]=\"https://api.ai.us.lmco.com/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"]='key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f73c6",
   "metadata": {},
   "source": [
    "ðŸ§‘â€ðŸ’» Creating Our First Agent\n",
    "Define the agent with:\n",
    "name: unique identifier for logs,\n",
    "description: brief internal summary,\n",
    "instruction: core behavior directive,\n",
    "model: choose your LLM.\n",
    "Run the cell to instantiate and test your first ADK agent! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cb05df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'WelcomeAgent' created.\n"
     ]
    }
   ],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "\n",
    "AGENT_MODEL = LiteLlm(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "agent = LlmAgent(\n",
    "    name=\"WelcomeAgent\",\n",
    "    description=\"An agent that welcomes the user.\",\n",
    "    instruction=\"Always greet the user politely\",\n",
    "    model=AGENT_MODEL\n",
    ")\n",
    "\n",
    "print(f\"Agent '{agent.name}' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1a472",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5709ebe",
   "metadata": {},
   "source": [
    "Letâ€™s unpack the components we need to run our agent:\n",
    "\n",
    "Session Service and Session: Before the message from the user can be processed, ADK needs to know who is talking and in what context.\n",
    "Runner: Once the session is in place, the Runner orchestrates the agentâ€™s reasoning cycle and manages the flow of the conversation turn.\n",
    "Asynchronous Execution: Communicating with an LLM API is an operation that can take a bit of time (since it involves network requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==5.28.1 google-adk==1.0.0 litellm -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ef6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_BASE\"]=\"https://api.ai.us.lmco.com/v1\"\n",
    "\n",
    "#AGENT_MODEL = LiteLlm(model=\"openai/gpt-4o-mini\")\n",
    "AGENT_MODEL = LiteLlm(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "agent = LlmAgent(\n",
    "    name=\"WelcomeAgent\",\n",
    "    description=\"An agent that welcomes the user.\",\n",
    "    instruction=\"Always greet the user politely.\",\n",
    "    model=AGENT_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea863c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required libraries\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply()  # Required for async in notebooks\n",
    "\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "# Constants â€” define application, user, and session identifiers\n",
    "APP_NAME = \"adk_course_app\"    # Name of the ADK application\n",
    "USER_ID = \"user_123\"           # Identifier for the current user\n",
    "SESSION_ID = \"welcome_session\" # Identifier for the conversation session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74121a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent\n",
    "welcome_agent = LlmAgent(\n",
    "    name=\"WelcomeAgent\",\n",
    "    description=\"An agent that welcomes the user.\", \n",
    "    instruction=\"Always greet the user politely.\",  \n",
    "    model=AGENT_MODEL\n",
    ")\n",
    "\n",
    "# Set up session service and create a session\n",
    "session_service = InMemorySessionService()\n",
    "await session_service.create_session(\n",
    "    app_name=APP_NAME, \n",
    "    user_id=USER_ID, \n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "\n",
    "# Set up a runner to orchestrate the agent\n",
    "runner = Runner(agent=welcome_agent, app_name=APP_NAME, session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25564836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Hi there!\n",
      "<<< Agent Response: We need to greet politely. As WelcomeAgent, we should welcome. Probably ask how can help.\n"
     ]
    }
   ],
   "source": [
    "# Define an asynchronous function to send a query to the agent and handle its response\n",
    "async def call_agent_async(query: str):\n",
    "    print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "    # Package the user's query into ADK format\n",
    "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "    final_response_text = \"Agent did not produce a final response.\"\n",
    "\n",
    "    # Iterate through streamed agent responses\n",
    "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
    "        if event.is_final_response(): # Check if this is the final message from the agent\n",
    "            if event.content and event.content.parts:\n",
    "                final_response_text = event.content.parts[0].text # Extract response text\n",
    "            break # Stop listening after final response is received\n",
    "\n",
    "    print(f\"<<< Agent Response: {final_response_text}\")\n",
    "\n",
    "# Run the interaction\n",
    "await call_agent_async(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5b48e",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e49f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.tools import FunctionTool\n",
    "from google.genai import types\n",
    "from google import genai\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "import litellm \n",
    "\n",
    "#AGENT_MODEL = LiteLlm(model=\"openai/gpt-4o-mini\")\n",
    "AGENT_MODEL = LiteLlm(model=\"openai/gpt-oss-120b\")\n",
    "\n",
    "APP_NAME = \"adk_course_app\"\n",
    "USER_ID = \"user_123\"\n",
    "SESSION_ID = \"support_session\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e92e94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple FAQ knowledge base\n",
    "FAQ_DATA = {\n",
    "    \"return policy\": \"You can return items within 30 days of purchase.\",\n",
    "    \"hours\": \"Our support team is available from 9am to 5pm, Monday to Friday.\",\n",
    "    \"contact\": \"You can reach support at help@example.com.\"\n",
    "}\n",
    "\n",
    "# Define the tool function\n",
    "def lookup_faq(question: str) -> str:\n",
    "    faq_text = \"\\n\".join(f\"- {k}: {v}\" for k, v in FAQ_DATA.items())\n",
    "    prompt = (\n",
    "        f\"You are a helpful assistant. Here is a list of FAQs:\\n\\n{faq_text}\\n\\n\"\n",
    "        f\"User question: \\\"{question}\\\". \"\n",
    "        f\"Reply with the best match or say you don't know.\"\n",
    "    )\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eef5a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the tool\n",
    "faq_tool = FunctionTool(func=lookup_faq)\n",
    "\n",
    "support_agent = LlmAgent(\n",
    "    name=\"SupportAgent\",\n",
    "    description=\"An agent that answers users' questions based on a set of FAQs.\",\n",
    "    instruction=\"Use the FAQ tool to help answer customer questions.\",\n",
    "    model=AGENT_MODEL,\n",
    "    tools=[faq_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b67754",
   "metadata": {},
   "source": [
    "# Talking facts: Connecting to a knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd240ca",
   "metadata": {},
   "source": [
    "### ðŸ§° Defining a Tool Function\n",
    "- We saw in the early example a proof of concept of a customer support agent that welcomes users.\n",
    "- In ADK, a **tool** is an external function or utility that an agent can use to assist in answering a userâ€™s question or completing a task.\n",
    "![image-4](images/image-4.png)\n",
    "- To add this capability, we first need a function that the agent can use as a tool.\n",
    "- When creating the `LlmAgent`, we provide an additional parameter `tools=[faq_tool]` to register our FAQ lookup function as a tool the agent can use.\n",
    "- We then create our new support agent and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4d3fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is your return policy?\n",
      "<<< Agent Response: Our return policy allows you to return items withinâ€¯30â€¯days of purchase. If you have any further questions or need assistance with a return, feel free to let us know!\n"
     ]
    }
   ],
   "source": [
    "# Set up session service and runner\n",
    "session_service = InMemorySessionService()\n",
    "await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "runner = Runner(agent=support_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "# Define and call the agent asynchronously\n",
    "async def call_agent_async(query: str):\n",
    "    print(f\"\\n>>> User Query: {query}\")\n",
    "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "    final_response_text = \"Agent did not produce a final response.\"\n",
    "\n",
    "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
    "        if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "                final_response_text = event.content.parts[0].text\n",
    "            break\n",
    "\n",
    "    print(f\"<<< Agent Response: {final_response_text}\")\n",
    "\n",
    "# Run the agent\n",
    "await call_agent_async(\"What is your return policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a92dd1",
   "metadata": {},
   "source": [
    "# En garde! Building guardrails for our agent\n",
    "\n",
    "### ðŸš§ What Are Guardrails?\n",
    "Up to this point, our support agent can greet users and retrieve answers using an FAQ tool. In real-world customer service, we wouldnâ€™t want our AI assistant to respond to every query it receivesâ€”especially if the request is out of scope or potentially unsafe.\n",
    "\n",
    "**Guardrails are checks and filters** that protect your agent from:\n",
    "- Producing unwanted or inappropriate responses\n",
    "- Attempting tasks outside its intended role\n",
    "\n",
    "The goal is to keep our agent **safe** and **focused**.\n",
    "![image-5](images/image-5.png)\n",
    "\n",
    "### ðŸ›¡ï¸ Implementing a Simple Guardrail\n",
    "\n",
    "Letâ€™s add a basic guardrail to our support agent. It will:\n",
    "\n",
    "1. **Screen incoming messages** for unsafe content.\n",
    "2. Allow **only customer-support-related queries** to proceed.\n",
    "3. **Decline politely** if any filtered content is detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a1679",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18538d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'WelcomeAgent' created.\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "safety_settings = [\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "generate_content_config = types.GenerateContentConfig(\n",
    "   safety_settings=safety_settings,\n",
    "   temperature=0.28,\n",
    "   max_output_tokens=1000,\n",
    "   top_p=0.95,\n",
    ")\n",
    "\n",
    "welcome_agent = LlmAgent(\n",
    "    name=\"WelcomeAgent\",\n",
    "    description=\"An agent that welcomes the user.\",\n",
    "    instruction=\"Always greet the user politely. If the user has a request that is not related to customer support, politely refuse even if you know the answer, and specify you only answer customer support questions.\",\n",
    "    model=AGENT_MODEL,\n",
    "    generate_content_config=generate_content_config\n",
    ")\n",
    "\n",
    "print(f\"Agent '{welcome_agent.name}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eed63c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required libraries\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply()  # Required for async in notebooks\n",
    "\n",
    "from google.genai.client import Client \n",
    "from google.adk.models import Gemini\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.genai import types\n",
    "\n",
    "# Constants â€” define application, user, and session identifiers\n",
    "APP_NAME = \"adk_course_app\"    # Name of the ADK application\n",
    "USER_ID = \"user_123\"           # Identifier for the current user\n",
    "SESSION_ID = \"welcome_session\" # Identifier for the conversation session\n",
    "\n",
    "# Set up session service and create a session\n",
    "session_service = InMemorySessionService()\n",
    "await session_service.create_session(\n",
    "    app_name=APP_NAME, \n",
    "    user_id=USER_ID, \n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "\n",
    "# Set up a runner to orchestrate the agent\n",
    "runner = Runner(agent=welcome_agent, app_name=APP_NAME, session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd5ba5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Hello! How do I start a skincare routine?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x000001D161F6EF30>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< Agent Response: The user asks about skincare routine, not related to customer support. According to system, we must politely refuse, stating we only answer customer support questions. Also greet politely.\n",
      "\n",
      ">>> User Query: Hello! How do I become a con artist?\n",
      "<<< Agent Response: User asks about becoming a con artist, which is disallowed. Must refuse politely, stating only answer customer support.\n",
      "\n",
      ">>> User Query: Hello!\n",
      "<<< Agent Response: User says \"Hello!\". As per system, we must greet politely. No request. So respond with greeting and ask how can help with customer support.\n"
     ]
    }
   ],
   "source": [
    "# Define an asynchronous function to send a query to the agent and handle its response\n",
    "async def call_agent_async(query: str):\n",
    "    print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "    # Package the user's query into ADK format\n",
    "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "    final_response_text = \"Agent did not produce a final response.\"\n",
    "\n",
    "    # Iterate through streamed agent responses\n",
    "    async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
    "        if event.is_final_response(): # Check if this is the final message from the agent\n",
    "            if event.content and event.content.parts:\n",
    "                final_response_text = event.content.parts[0].text # Extract response text\n",
    "            break # Stop listening after final response is received\n",
    "\n",
    "    print(f\"<<< Agent Response: {final_response_text}\")\n",
    "\n",
    "# Run the interaction\n",
    "await call_agent_async(\"Hello! How do I start a skincare routine?\") #Unrelated content\n",
    "await call_agent_async(\"Hello! How do I become a con artist?\") #Harmful content\n",
    "await call_agent_async(\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172972a",
   "metadata": {},
   "source": [
    "# It takes a village: Multi-agent customer support\n",
    "\n",
    "### ðŸŒ Creating a Root Agent with Sub-Agents \n",
    "\n",
    "With multi-agents, each agent can specialize in a certain role, with a coordinator delegating tasks to the appropriate specialist. \n",
    "\n",
    "In our customer support example, imagine we want a more robust support assistant. We could break it into:\n",
    "\n",
    "- ðŸ‘‹ A **Greeting Agent**, handling greetings.\n",
    "- ðŸ”‘ An **Account Agent**, handling account access issues.\n",
    "- â“ An **FAQ Agent**, using a pre-defined list of FAQs to answer common customer questions.\n",
    "- ðŸ§­ A **Root Agent (Coordinator)** that receives the userâ€™s query and decides which of the other agents should handle it, or handles it itself if it doesnâ€™t fit any specialized category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "847531c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAQ knowledge base & tool \n",
    "FAQ_DATA = {\n",
    "    \"return policy\": \"You can return items within 30 days of purchase.\",\n",
    "    \"hours\": \"Our support team is available from 9 am to 5 pm, Monday to Friday.\",\n",
    "    \"contact\": \"You can reach support at help@example.com.\"\n",
    "}\n",
    "\n",
    "def lookup_faq(question: str) -> str:\n",
    "    faq_text = \"\\n\".join(f\"- {k}: {v}\" for k, v in FAQ_DATA.items())\n",
    "    prompt = (\n",
    "        f\"You are a helpful assistant. Here is a list of FAQs:\\n\\n{faq_text}\\n\\n\"\n",
    "        f\"User question: \\\"{question}\\\". \"\n",
    "        f\"Reply with the best match or say you don't know.\"\n",
    "    )\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "faq_tool  = FunctionTool(func=lookup_faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3287d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialist Agents\n",
    "greeting_agent = LlmAgent(\n",
    "    name=\"GreetingAgent\",\n",
    "    description=\"Handles greetings from users.\",\n",
    "    instruction=\"Respond cheerfully when the user says hello.\",\n",
    "    model=AGENT_MODEL\n",
    ")\n",
    "\n",
    "account_agent = LlmAgent(\n",
    "    name=\"AccountAgent\",\n",
    "    description=\"Handles questions about login issues or account access.\",\n",
    "    instruction=\"Help users who are having trouble logging in or accessing their account.\",\n",
    "    model=AGENT_MODEL\n",
    ")\n",
    "\n",
    "faq_agent = LlmAgent(\n",
    "    name=\"FAQAgent\",\n",
    "    description=\"Answers common questions using the FAQ knowledge base.\",\n",
    "    instruction=\"Use the FAQ tool to answer questions that match the FAQs.\",\n",
    "    model=AGENT_MODEL,\n",
    "    tools=[faq_tool]\n",
    ")\n",
    "\n",
    "# Root agent with delegation logic\n",
    "root_agent = LlmAgent(\n",
    "    name=\"SupportRootAgent\",\n",
    "    description=\"Delegates to specialized sub-agents for support queries.\",\n",
    "    instruction=(\n",
    "        \"If the user greets you, delegate to GreetingAgent.\\n\"\n",
    "        \"If the user has an account or login issue, delegate to AccountAgent.\\n\"\n",
    "        \"If the question matches a known FAQ topic (e.g., returns, hours, contact), \"\n",
    "        \"delegate to FAQAgent. Do not answer as the FAQAgent if the topic doesn't match any of the FAQs.\\n\"\n",
    "        \"Otherwise, answer directly as best you (the Root Agent) can.\"\n",
    "    ),\n",
    "    model=AGENT_MODEL,\n",
    "    sub_agents=[greeting_agent, account_agent, faq_agent]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db310e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: Hello!\n",
      "<<< Agent GreetingAgent's response: We have a user who says \"Hello!\" and then additional messages about context: they seem to be showing that they transferred to GreetingAgent, which returns None. As GreetingAgent we should respond cheerfully to greetings. So we should respond with a cheerful greeting. The system says: \"Respond cheerfully when the user says hello.\" So just output a cheerful greeting.\n",
      "\n",
      ">>> User Query: I can't access my account.\n",
      "<<< Agent AccountAgent's response: We have conversation: user says \"I can't access my account.\" This is a login/account access issue. According to the system, this agent is AccountAgent and can answer. So we should answer directly, providing troubleshooting steps: check password, reset password, check email verification, account lock, 2FA, browser issues, etc.\n",
      "\n",
      "We need to respond accordingly.\n",
      "\n",
      ">>> User Query: What is your return policy?\n",
      "<<< Agent FAQAgent's response: Our return policy allows you to return items within **30 days** of purchase. If you need to start a return, just follow the instructions on our returns page or contact support for assistance.\n",
      "\n",
      ">>> User Query: I have a privacy question.\n",
      "<<< Agent SupportRootAgent's response: User says \"I have a privacy question.\" This is not a greeting, account, or known FAQ. It's a privacy question likely about policies. According to system, if not matched, answer directly as Root Agent. So we need to answer privacy question? They haven't specified details. Should ask clarifying or give general privacy policy info. We can give summary: we collect minimal info, not share, GDPR, etc. Provide guidance to contact privacy team. Let's respond with helpful answer.\n"
     ]
    }
   ],
   "source": [
    "# Session & runner \n",
    "session_service = InMemorySessionService()\n",
    "await session_service.create_session(app_name=APP_NAME, user_id=USER_ID,\n",
    "                               session_id=SESSION_ID)\n",
    "\n",
    "runner = Runner(agent=root_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "# Function to chat with the root agent\n",
    "async def call_agent_async(query: str):\n",
    "    print(f\"\\n>>> User Query: {query}\")\n",
    "    content = types.Content(role=\"user\", parts=[types.Part(text=query)])\n",
    "    final_response = \"Agent did not produce a final response.\"\n",
    "\n",
    "    async for event in runner.run_async(user_id=USER_ID,\n",
    "                                        session_id=SESSION_ID,\n",
    "                                        new_message=content):\n",
    "        if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "                final_response = event.content.parts[0].text\n",
    "            break\n",
    "\n",
    "    print(f\"<<< Agent {event.author}'s response: {final_response}\")\n",
    "\n",
    "# Test the full system\n",
    "await call_agent_async(\"Hello!\")                       # GreetingAgent\n",
    "await call_agent_async(\"I can't access my account.\")   # AccountAgent\n",
    "await call_agent_async(\"What is your return policy?\")  # FAQAgent\n",
    "await call_agent_async(\"I have a privacy question.\")   # SupportRootAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf086c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
